{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "min_error = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AutoEncoder:\n",
    "    def __init__(self, input_size, hidden_size, activation_function, previous=None, outputX=None, inputX=None):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.tf_session = None\n",
    "\n",
    "        self.weight_e = tf.Variable(tf.random_normal([input_size, hidden_size]))\n",
    "        self.bias_e = tf.Variable(tf.random_normal([hidden_size]))\n",
    "\n",
    "        if previous:\n",
    "            self.previous = previous\n",
    "        else:\n",
    "            self.previous = None\n",
    "\n",
    "        if inputX:\n",
    "            self.inputX = inputX\n",
    "        else:\n",
    "            self.inputX = tf.placeholder('float', [None, input_size])\n",
    "\n",
    "        if outputX!= None :\n",
    "            self.outputX = outputX\n",
    "        else:\n",
    "            self.outputX = self.inputX\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "        self.encoder = activation_function(tf.add(tf.matmul(self.inputX, self.weight_e), self.bias_e))\n",
    "\n",
    "        self.weight_d = tf.transpose(self.weight_e)\n",
    "        self.bias_d = tf.Variable(tf.random_normal([input_size]))\n",
    "\n",
    "        self.decoder = activation_function(tf.add(tf.matmul(self.encoder, self.weight_d), self.bias_d))\n",
    "\n",
    "    def set_weights_biases(self, weight_e, bias_e, bias_d, inputX=None):\n",
    "        self.weight_e = weight_e\n",
    "        self.weight_d = tf.transpose(weight_e)\n",
    "        self.bias_e = bias_e\n",
    "        self.bias_d = bias_d\n",
    "        if inputX:\n",
    "            self.encoder = self.activation_function(tf.add(tf.matmul(inputX, self.weight_e), self.bias_e))\n",
    "        else:\n",
    "            self.encoder = self.activation_function(tf.add(tf.matmul(self.inputX, self.weight_e), self.bias_e))\n",
    "\n",
    "        self.decoder = self.activation_function(tf.add(tf.matmul(self.encoder, self.weight_d), self.bias_d))\n",
    "\n",
    "    def get_trained_values(self, inputX=None):\n",
    "        act_weight_e = tf.constant(self.tf_session.run(self.weight_e))\n",
    "        act_bias_e = tf.constant(self.tf_session.run(self.bias_e))\n",
    "\n",
    "        act_bias_d = tf.constant(self.tf_session.run(self.bias_d))\n",
    "\n",
    "        act_auto_encoder = AutoEncoder(self.input_size, self.hidden_size, self.activation_function, self.previous)\n",
    "        act_auto_encoder.set_weights_biases(act_weight_e, act_bias_e, act_bias_d, inputX)\n",
    "\n",
    "        return act_auto_encoder\n",
    "\n",
    "    def set_constants(self, inputX=None):\n",
    "        self.weight_e = tf.constant(self.tf_session.run(self.weight_e))\n",
    "        self.bias_e = tf.constant(self.tf_session.run(self.bias_e))\n",
    "        if inputX:\n",
    "            self.inputX = inputX\n",
    "\n",
    "        self.encoder = self.activation_function(tf.add(tf.matmul(self.inputX, self.weight_e), self.bias_e))\n",
    "\n",
    "        self.weight_d = tf.constant(self.tf_session.run(self.weight_d))\n",
    "        self.bias_d = tf.constant(self.tf_session.run(self.bias_d))\n",
    "\n",
    "        self.decoder = self.activation_function(tf.add(tf.matmul(self.encoder, self.weight_d), self.bias_d))\n",
    "\n",
    "    def output(self, input_data, session=None):\n",
    "        if session:\n",
    "            self.tf_session = session\n",
    "\n",
    "        if self.previous:\n",
    "            input_data = self.previous.output(input_data, self.tf_session)\n",
    "\n",
    "        return self.tf_session.run(self.encoder, feed_dict={self.inputX:input_data})\n",
    "\n",
    "    def unsupervised_train(self, input_data, training_epochs=20, output_data = [], learning_rate=0.01, display_steps=10, batch_size=256):\n",
    "\n",
    "        print(len(output_data))\n",
    "        if len(output_data) == 0:\n",
    "            y_pred = self.decoder\n",
    "        else:\n",
    "            y_pred = self.encoder\n",
    "        y_true = self.outputX\n",
    "\n",
    "        cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "        with tf.Session() as self.tf_session:\n",
    "\n",
    "            input_size = len(input_data)\n",
    "            self.tf_session.run(tf.global_variables_initializer())\n",
    "            total_batch = int(input_size / batch_size)\n",
    "            for epoch in range(training_epochs):\n",
    "                start = time.time()\n",
    "                count = 0\n",
    "                cst = 0.0\n",
    "\n",
    "                for i in range(total_batch):\n",
    "                    end = count + batch_size\n",
    "                    if end > input_size:\n",
    "                        end = input_size\n",
    "\n",
    "                    if self.previous:\n",
    "                        batch_xs = self.previous.output(input_data[count:end], self.tf_session)\n",
    "                    else:\n",
    "                        batch_xs = input_data[count:end]\n",
    "\n",
    "                    if len(output_data) == 0:\n",
    "                        batch_ys = batch_xs\n",
    "                    else:\n",
    "                        batch_ys = output_data[count:end]\n",
    "\n",
    "                    _, c = self.tf_session.run([optimizer, cost],\n",
    "                                               feed_dict={self.inputX: batch_xs, self.outputX: batch_ys})\n",
    "                    cst = cst + c\n",
    "                    count = end\n",
    "\n",
    "                cst = cst / total_batch\n",
    "                if cst < min_error:\n",
    "                    break\n",
    "\n",
    "                end = time.time()\n",
    "                if epoch % display_steps == 0:\n",
    "                    print(\"Epoch:\", '%04d' % (epoch + 1),\n",
    "                          \"cost=\", \"{:.9f}\".format(cst))\n",
    "                    print('epoch took {}'.format((end - start) * 1000))\n",
    "\n",
    "\n",
    "            print(\"Optimization Finished!\")\n",
    "\n",
    "            self.set_constants()\n",
    "\n",
    "            return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergeLayers(layers):\n",
    "    inputX = layers[0].inputX\n",
    "    for layer in layers:\n",
    "        inputX = layer.activation_function(tf.add(tf.matmul(inputX, layer.weight_e), layer.bias_e))\n",
    "\n",
    "    layers.reverse()\n",
    "\n",
    "    output = inputX\n",
    "    for layer in layers:\n",
    "        output = layer.activation_function(tf.add(tf.matmul(output, layer.weight_d), layer.bias_d))\n",
    "\n",
    "    layers.reverse()\n",
    "    return output, layers[0].inputX\n",
    "\n",
    "def getEncoder(layers):\n",
    "    inputX = layers[0].inputX\n",
    "    for layer in layers:\n",
    "        inputX = layer.activation_function(tf.add(tf.matmul(inputX, layer.weight_e), layer.bias_e))\n",
    "\n",
    "\n",
    "    return inputX, layers[0].inputX\n",
    "\n",
    "def finalLayer(layers):\n",
    "    inputX = layers[0].inputX\n",
    "    for layer in layers:\n",
    "        weight = tf.Variable(layer.weight_e)\n",
    "        bias = tf.Variable(layer.bias_e)\n",
    "        inputX = layer.activation_function(tf.add(tf.matmul(inputX, weight), bias))\n",
    "\n",
    "    return inputX, layers[0].inputX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_layer_train(encoder_pt, input_count, hidden_size, input_data, output_data, training_epochs=20, learning_rate=0.01, display_steps=10, batch_size=256):\n",
    "    weight = tf.Variable(tf.random_normal([input_count, hidden_size]))\n",
    "    bias = tf.Variable(tf.random_normal([hidden_size]))\n",
    "\n",
    "    encoder = tf.nn.sigmoid(tf.add(tf.matmul(encoder_pt, weight), bias))\n",
    "\n",
    "    y_true = outputX\n",
    "    y_pred = encoder\n",
    "\n",
    "    cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    input_size =len(input_data)\n",
    "\n",
    "    with tf.Session() as def_session:\n",
    "        def_session.run(tf.global_variables_initializer())\n",
    "        for epoch in range(training_epochs):\n",
    "            total_batch = int(input_size / batch_size)\n",
    "            count = 0\n",
    "            cst = 0.0\n",
    "            for i in range(total_batch):\n",
    "                end = count + batch_size\n",
    "                if end > input_size:\n",
    "                    end = input_size\n",
    "\n",
    "                batch_xs = input_data[count:end]\n",
    "                batch_ys = output_data[count:end]\n",
    "\n",
    "                _, c = def_session.run([optimizer, cost],\n",
    "                                       feed_dict={inputX: batch_xs, outputX: batch_ys})\n",
    "\n",
    "                cst += c\n",
    "                count = end\n",
    "            cst = cst / total_batch\n",
    "            if cst < min_error:\n",
    "                break\n",
    "\n",
    "            if epoch % display_steps == 0:\n",
    "                print(\"Epoch:\", '%04d' % (epoch + 1),\n",
    "                      \"cost=\", \"{:.9f}\".format(cst))\n",
    "\n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(encoder, 1), tf.argmax(outputX, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        print(def_session.run(accuracy, feed_dict={inputX: mnist.test.images, outputX: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch: 0001 cost= 0.336672079\n",
      "epoch took 2100.295066833496\n",
      "Epoch: 0011 cost= 0.014855703\n",
      "epoch took 395.5693244934082\n",
      "Epoch: 0021 cost= 0.009234981\n",
      "epoch took 413.10977935791016\n",
      "Epoch: 0031 cost= 0.005646397\n",
      "epoch took 399.7480869293213\n",
      "Epoch: 0041 cost= 0.004220703\n",
      "epoch took 396.0583209991455\n",
      "Epoch: 0051 cost= 0.003866621\n",
      "epoch took 398.86474609375\n",
      "Epoch: 0061 cost= 0.003619733\n",
      "epoch took 392.5304412841797\n",
      "Epoch: 0071 cost= 0.003443278\n",
      "epoch took 417.54984855651855\n",
      "Epoch: 0081 cost= 0.003307761\n",
      "epoch took 408.0851078033447\n",
      "Epoch: 0091 cost= 0.003197184\n",
      "epoch took 413.7580394744873\n",
      "Epoch: 0101 cost= 0.003100779\n",
      "epoch took 399.57404136657715\n",
      "Epoch: 0111 cost= 0.003024038\n",
      "epoch took 392.5774097442627\n",
      "Epoch: 0121 cost= 0.002958278\n",
      "epoch took 420.590877532959\n",
      "Epoch: 0131 cost= 0.002893918\n",
      "epoch took 409.1513156890869\n",
      "Epoch: 0141 cost= 0.002842205\n",
      "epoch took 406.10241889953613\n",
      "Epoch: 0151 cost= 0.002791447\n",
      "epoch took 404.06227111816406\n",
      "Optimization Finished!\n",
      "0\n",
      "Epoch: 0001 cost= 0.226607136\n",
      "epoch took 474.02358055114746\n",
      "Epoch: 0011 cost= 0.021412760\n",
      "epoch took 450.6223201751709\n",
      "Epoch: 0021 cost= 0.017457954\n",
      "epoch took 445.27339935302734\n",
      "Epoch: 0031 cost= 0.016537499\n",
      "epoch took 440.1068687438965\n",
      "Epoch: 0041 cost= 0.015912326\n",
      "epoch took 436.7027282714844\n",
      "Epoch: 0051 cost= 0.015298241\n",
      "epoch took 446.17152214050293\n",
      "Epoch: 0061 cost= 0.014824919\n",
      "epoch took 443.026065826416\n",
      "Epoch: 0071 cost= 0.014394220\n",
      "epoch took 437.64472007751465\n",
      "Epoch: 0081 cost= 0.014048073\n",
      "epoch took 440.7026767730713\n",
      "Epoch: 0091 cost= 0.013799249\n",
      "epoch took 443.6829090118408\n",
      "Epoch: 0101 cost= 0.013582639\n",
      "epoch took 439.6977424621582\n",
      "Epoch: 0111 cost= 0.013338138\n",
      "epoch took 512.6407146453857\n",
      "Epoch: 0121 cost= 0.013057535\n",
      "epoch took 552.6902675628662\n",
      "Epoch: 0131 cost= 0.012863417\n",
      "epoch took 452.98171043395996\n",
      "Epoch: 0141 cost= 0.012711003\n",
      "epoch took 448.2591152191162\n",
      "Epoch: 0151 cost= 0.012529548\n",
      "epoch took 439.1458034515381\n",
      "Optimization Finished!\n",
      "0\n",
      "Epoch: 0001 cost= 0.255061746\n",
      "epoch took 555.6108951568604\n",
      "Epoch: 0011 cost= 0.018688668\n",
      "epoch took 528.7747383117676\n",
      "Epoch: 0021 cost= 0.017307538\n",
      "epoch took 532.2685241699219\n",
      "Epoch: 0031 cost= 0.016542198\n",
      "epoch took 531.3036441802979\n",
      "Epoch: 0041 cost= 0.016058181\n",
      "epoch took 571.7687606811523\n",
      "Epoch: 0051 cost= 0.015696710\n",
      "epoch took 564.763069152832\n",
      "Epoch: 0061 cost= 0.015423467\n",
      "epoch took 547.5606918334961\n",
      "Epoch: 0071 cost= 0.015241675\n",
      "epoch took 545.5436706542969\n",
      "Epoch: 0081 cost= 0.015098745\n",
      "epoch took 543.938398361206\n",
      "Epoch: 0091 cost= 0.014963000\n",
      "epoch took 573.1792449951172\n",
      "Epoch: 0101 cost= 0.014867914\n",
      "epoch took 568.7210559844971\n",
      "Epoch: 0111 cost= 0.014780044\n",
      "epoch took 532.5367450714111\n",
      "Epoch: 0121 cost= 0.014704045\n",
      "epoch took 534.6677303314209\n",
      "Epoch: 0131 cost= 0.014654068\n",
      "epoch took 549.1917133331299\n",
      "Epoch: 0141 cost= 0.014612531\n",
      "epoch took 550.0352382659912\n",
      "Epoch: 0151 cost= 0.014563063\n",
      "epoch took 546.6828346252441\n",
      "Optimization Finished!\n",
      "0\n",
      "Epoch: 0001 cost= 0.217169197\n",
      "epoch took 641.3214206695557\n",
      "Epoch: 0011 cost= 0.011089514\n",
      "epoch took 638.8020515441895\n",
      "Epoch: 0021 cost= 0.010394348\n",
      "epoch took 632.8108310699463\n",
      "Epoch: 0031 cost= 0.009715588\n",
      "epoch took 625.7812976837158\n",
      "Epoch: 0041 cost= 0.009243725\n",
      "epoch took 666.386604309082\n",
      "Epoch: 0051 cost= 0.008926608\n",
      "epoch took 900.1474380493164\n",
      "Epoch: 0061 cost= 0.008664218\n",
      "epoch took 625.8368492126465\n",
      "Epoch: 0071 cost= 0.008434375\n",
      "epoch took 700.4122734069824\n",
      "Epoch: 0081 cost= 0.008206005\n",
      "epoch took 611.6023063659668\n",
      "Epoch: 0091 cost= 0.008020091\n",
      "epoch took 622.8888034820557\n",
      "Epoch: 0101 cost= 0.007856220\n",
      "epoch took 619.7688579559326\n",
      "Epoch: 0111 cost= 0.007735798\n",
      "epoch took 621.3607788085938\n",
      "Epoch: 0121 cost= 0.007594512\n",
      "epoch took 626.2826919555664\n",
      "Epoch: 0131 cost= 0.007443870\n",
      "epoch took 631.3779354095459\n",
      "Epoch: 0141 cost= 0.007340593\n",
      "epoch took 618.685245513916\n",
      "Epoch: 0151 cost= 0.007237145\n",
      "epoch took 618.7763214111328\n",
      "Optimization Finished!\n",
      "0\n",
      "Epoch: 0001 cost= 0.209687919\n",
      "epoch took 775.2747535705566\n",
      "Epoch: 0011 cost= 0.028648314\n",
      "epoch took 707.41868019104\n",
      "Epoch: 0021 cost= 0.027699278\n",
      "epoch took 695.0459480285645\n",
      "Epoch: 0031 cost= 0.027051080\n",
      "epoch took 692.6374435424805\n",
      "Epoch: 0041 cost= 0.026668653\n",
      "epoch took 946.6829299926758\n",
      "Epoch: 0051 cost= 0.026351336\n",
      "epoch took 689.4054412841797\n",
      "Epoch: 0061 cost= 0.026076051\n",
      "epoch took 690.0238990783691\n",
      "Epoch: 0071 cost= 0.025868198\n",
      "epoch took 690.5398368835449\n",
      "Epoch: 0081 cost= 0.025730962\n",
      "epoch took 702.7084827423096\n",
      "Epoch: 0091 cost= 0.025590851\n",
      "epoch took 686.4759922027588\n",
      "Epoch: 0101 cost= 0.025458199\n",
      "epoch took 733.5736751556396\n",
      "Epoch: 0111 cost= 0.025376773\n",
      "epoch took 683.9721202850342\n",
      "Epoch: 0121 cost= 0.025300750\n",
      "epoch took 739.9992942810059\n",
      "Epoch: 0131 cost= 0.025208710\n",
      "epoch took 702.0385265350342\n",
      "Epoch: 0141 cost= 0.025072537\n",
      "epoch took 691.5559768676758\n"
     ]
    }
   ],
   "source": [
    "outputX = tf.placeholder('float', [None, 10])\n",
    "layer1 = AutoEncoder(784, 256, tf.nn.sigmoid)\n",
    "layer2 = AutoEncoder(256, 256, tf.nn.sigmoid, layer1)\n",
    "layer3 = AutoEncoder(256, 128, tf.nn.sigmoid, layer2)\n",
    "layer4 = AutoEncoder(128, 128, tf.nn.sigmoid, layer3)\n",
    "layer5 = AutoEncoder(128, 64, tf.nn.sigmoid, layer4)\n",
    "#layer4 = AutoEncoder(128, 10, tf.nn.sigmoid, layer3, outputX)\n",
    "\n",
    "\n",
    "examples_to_show = 10\n",
    "\n",
    "layers = []\n",
    "\n",
    "layers.append(layer1.unsupervised_train(mnist.train.images, 160))\n",
    "layers.append(layer2.unsupervised_train(mnist.train.images, 160))\n",
    "layers.append(layer3.unsupervised_train(mnist.train.images, 160))\n",
    "layers.append(layer4.unsupervised_train(mnist.train.images, 160))\n",
    "layers.append(layer5.unsupervised_train(mnist.train.images, 160))\n",
    "#layers.append(layer4.unsupervised_train(mnist.train.images, 320, mnist.train.labels))\n",
    "\n",
    "decoder, input = mergeLayers(layers)\n",
    "\n",
    "#encoder, inputX = getEncoder(layers)\n",
    "\n",
    "encoder_pt, inputX = finalLayer(layers)\n",
    "\n",
    "final_layer_train(encoder_pt, 64, 10, mnist.train.images, mnist.train.labels, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as def_session:\n",
    "    def_session.run(tf.global_variables_initializer())\n",
    "    encode_decode = def_session.run(decoder, feed_dict={input:mnist.test.images[:examples_to_show]})\n",
    "\n",
    "f, a = plt.subplots(2, 10, figsize=(10, 2))\n",
    "for i in range(examples_to_show):\n",
    "    a[0][i].imshow(np.reshape(mnist.test.images[i], (28, 28)))\n",
    "    a[1][i].imshow(np.reshape(encode_decode[i], (28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
